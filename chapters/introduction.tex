Python is a popular and versatile programming language that is distinguished by its interpreted nature, high-level syntax, and support for a variety of programming paradigms.
It makes use of dynamic typing and efficient memory management techniques, and its modular design makes it easy to extend and integrate programmable interfaces into pre-existing applications \cite{Python_language_wiki}.
Furthermore, Python provides a variety of dynamic features, such as object creation, attribute access, module loading, and function definition, making it appealing to developers of all skill levels. 

Recent studies have demonstrated Python's widespread usage in various applications, including analysis, bug detection, performance assessment, and vulnerability testing \cite{Python_usage_study1, Python_usage_study2}. This can be attributed to its accessible syntax, extensive community support, and comprehensive libraries. Python's robust support for data analysis, visualization, statistical analysis, and cross-platform compatibility renders it an exceptional choice for the development of cross-platform software and tools. Furthermore, Python is equipped with an assortment of testing and debugging tools, ranging from built-in frameworks like unittest and pytest to third-party tools such as PyCharm \cite{PyCharm_2023} and PyDev \cite{PyDev_2023} for debugging and code refactoring. Other such tools include Pynguin for test generation and DynaPyt for dynamic analysis. These tools aid developers in identifying and resolving bugs while optimizing code performance.

In the realm of high-level programming languages, Python, along with Java and C++, are frequently employed for their potency, object-oriented structure, and dynamic features, rendering them suitable for intricate software development applications. The dynamic features of Java encompass automatic memory management, garbage collection, reflections, and exceptions, while C++ offers templates, dynamic memory allocation, and runtime dynamic linking. To assess the dynamic features of these languages, dynamic benchmarks can be employed, which involve gauging the performance of a system or application under real-world conditions. Contrasting with static benchmarks that evaluate systems or applications under controlled circumstances, dynamic benchmarks endeavor to emulate actual usage patterns of users and applications to attain more accurate performance measurements. Java and C++ boast numerous dynamic benchmarks, such as Java Microbenchmark Harness (JMH) \cite{JMH}, The DaCapo Benchmark \cite{DaCapo_2006}, The Computer Language Benchmarks Game \cite{C++_Benchmark1}, and Boost.Benchmarks \cite{Boost_Benchmarks}. Although Python possesses some dynamic benchmarks, such as PyPerformance \cite{PyPerformance} and Apache Bench \cite{Apache_Benchmark}, they are not general-purpose benchmarks like those offered for Java and C++. Python benchmarks evaluate program performance for a single dynamic feature in a specific domain under real-world conditions.

While the existing python benchmarks evaluate single dynamic feature from a specific domain, a general-purpose benchmark would allow us to analyze the different dynamic features in the real world conditions. Such a benchmark would allow the evaluation of program performance from a wide range of application domains such as machine learning, web development, server developement and virtual environments, brought to the table due to the popularity and wide spread usage of python. With the general-purpose python dynamic benchmark one could generate run-time analysis using tools such as systrace \cite{systrace} and DynaPyt \cite{DynaPyt2022}. A general-purpose benchmark can provide training data for machine learning (ML) and deep learning (DL) tasks of software engineering such as bug detection \cite{DeepBugs2018}, code completion \cite{code_completion}, quality analysis \cite{Code_analysis_1, Code_analysis_2}, code refactoring \cite{code_refactoring}, and testing \cite{testing_1, testing_2, testing_3} which improve the productivity, efficiency and quality of code.

In this work, we provide a general-purpose dynamic benchmark of executable python software, named DyPyBench, which consists of 50 python projects including frameworks and libraries. The entire benchmark, encapsulated inside a docker container, can be setup using the docker image containing the projects, alongside the frameworks such as DynaPyt \cite{DynaPyt2022}, LExecutor \cite{LExecutor_2023} and PyCG \cite{PyCG_2021}. All of the python projects are open-sourced and available on github \cite{github} including the analysis frameworks mentioned above. The chosen projects are popular and belong to diverse application domains such as audio e.g., pydub, command-line tools e.g., thefuck, and computer vision e.g., scikit-learn. The benchmark provided in this work provides a single command line interface to run test suites and dynamic analysis using the DynaPyt framework. Using the same command line interface, DypyBench can run LExecutor to generate traces for training the neural model of LExecutor. PyCG, which helps in generating static call graphs can also be run using the command line interface. In essence this work aims to achieve the following features for the dynamic benchmark:
\begin{itemize}
    \item \textbf{Large-scale} The benchmark should comprise tens of real-world open-source projects, allowing users to evaluate their analyses on a wide range of code.
    \item \textbf{Diverse} The benchmark should contain projects from a diverse range of application domains that reflect the state of today's Python ecosystem.
    \item \textbf{Ready-to-run} There should be a single interface to run each project in the benchmark, making it easy for users to set up and execute the entire benchmark.
    \item \textbf{Ready-to-analyze} To enable dynamic analyses, the executions of all projects in the benchmark should be set up to be analyzed with the DynaPyt framework.
    \item \textbf{Compositional} To help users understand the behavior of specific projects or even individual test cases, it should be easy to run subsets of full benchmark.
    \item \textbf{Long-term} The benchmark should be built using commonly used tools and formats, e.g., pip and Docker, to ensure its longevity.
    \item \textbf{**Extensibility} The benchmark should be easily extensible, allowing users to add, remove or update the tools, frameworks and projects.  
\end{itemize}

To achieve the desired features, DyPyBench faced some obstacles. Python is a versatile language that is used in a wide range of applications, including machine learning, web development, and command-line interface development, among others. Many of these projects are open source and available on GitHub for community use. DyPyBench uses a selection of these popular and well-received projects, which are filtered using the open source project "awesome-python". This provides nearly 500 projects across different categories, representing the various application domains covered by Python. However, not all of these projects are widely accepted by the community, so DyPyBench applies a filter criterion based on a lower limit of Github stars.

DyPyBench is a dynamic benchmark that executes projects through their test suites, and the benchmark aims to use the popular pytest library, which is capable of running tests written using pytest and unittest. The selected projects have different dependencies, and DyPyBench handles this by creating virtual environments for each project with its necessary dependencies, as specified by a requirement file. In addition, some projects require additional libraries for executing the test suites, and these are added to the specific virtual environments. Operating system requirements also vary for certain projects, and this is managed by exposing the benchmark as a docker container with a Ubuntu base image and adding all the necessary OS dependencies to this image.

As the open source projects are continuously being developed and improved, the source code of the projects changes and the steps used for installing and setting up the project may vary. To address this, DyPyBench fixes a date for the code checkout from the GitHub repository. Setting up, installing, and executing each project individually is challenging due to the large number of projects in the benchmark. Therefore, DyPyBench automates these steps by performing common steps for each project using bash scripts, with exceptions specified where required for specific projects.

The analysis tools and frameworks provided by the benchmark have their own ways of execution, requiring the user to know the command for the specific tool or framework. To simplify this process, DyPyBench provides a single command-line interface that incorporates these tools and frameworks and uses Python scripts to accept arguments to perform tasks related to the specific tool or framework.
