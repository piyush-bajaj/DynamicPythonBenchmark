Python is a popular and versatile programming language that is distinguished by its interpreted nature, high-level syntax, and support for a variety of programming paradigms.
It makes use of dynamic typing and efficient memory management techniques, and its modular design makes it easy to extend and integrate programmable interfaces into pre-existing applications \cite{Python_language_wiki}.
Furthermore, Python provides a variety of dynamic features, such as object creation, attribute access, module loading, and function definition, making it appealing to developers of all skill levels.
Python also provides comprehensive libraries and has an extensive community support due to its popularity.

Recent studies have demonstrated Python's widespread usage in various applications, including analysis, bug detection, performance assessment, and vulnerability testing \cite{Python_usage_study1, Python_usage_study2, Python_usage3}.
Python is used to make cross-platform software and tools for data analysis, visualization, game development, and web development such as Instagram \cite{Insta_architecture}, Spotify \cite{Spotify_python}, and Galcon \cite{game_python} to name a few.
Scientific computing tasks such as simulations, modeling and numerical analysis are also built using Python \cite{python_scientific}.
Besides, Python can also be used for automating repetitive tasks such as software testing, web scraping, and data processing \cite{python_automate}.

In the realm of high-level programming languages, Python, along with Java and C++, are frequently employed for their potency, object-oriented structure, and dynamic features, rendering them suitable for intricate software development applications. 
To gauge the performance of a system or application under real-world conditions, we employ dynamic benchmarks to assess the dynamic features of these languages.
We employ benchmarks to gauge the performance and quality of a system or application.
There are two types of performance benchmarks, static and dynamic based on code execution.
Contrasting with static benchmarks that evaluate systems or applications under controlled circumstances, dynamic benchmarks endeavor to emulate actual usage patterns of users and applications to attain more accurate performance measurements.
There are numerous benchmarks available for Java, C++ and Python such as Java Microbenchmark Harness (JMH) \cite{JMH}, The DaCapo Benchmark \cite{DaCapo_2006}, The Computer Language Benchmarks Game \cite{C++_Benchmark1}, Boost.Benchmarks \cite{Boost_Benchmarks}, PyPerformance \cite{PyPerformance}, and Apache Bench \cite{Apache_Benchmark}. 
While Java and C++ boast numerous dynamic benchmarks to compare the runtime performance of software projects, Python lacks such dynamic benchmarks for complex applications.

The popularity and the dynamism of Python makes it a prime candidate for dynamic analyses to detect programming errors, security vulnerabilities, and performance issues. 
However, the lack of dynamic benchmarks for complex applications in Python hinders the development of dynamic analyses.
To study the performance of software projects in Python under real-world conditions, a dynamic benchmark would be able to provide a framework to perform dynamic analyses.
The dynamic benchmark would perform analyses using tools such as systrace \cite{systrace} and DynaPyt \cite{DynaPyt2022} covering a wide range of application domains provided by Python.
Besides being a framework for dynamic analyses, the Python benchmark would allow generation of training data for machine learning and deep learning tasks in software engineering such as bug detection \cite{DeepBugs2018}, quality analysis \cite{Code_analysis_1, Code_analysis_2}, code refactoring \cite{code_refactoring}, and testing \cite{testing_1, testing_2, testing_3} to improve the productivity, efficiency and quality of code.

In this work, we provide a general-purpose dynamic benchmark of executable python software. This benchmark is named DyPyBench and consists of 50 python projects including frameworks and libraries.
The entire benchmark, encapsulated inside a docker container, can be setup using the docker image containing the projects, alongside the analysis frameworks such as DynaPyt \cite{DynaPyt2022} and PyCG \cite{PyCG_2021} to perform dynamic and static analysis respectively. Additionally, the benchmark also integrates LExecutor \cite{LExecutor_2023}, which is a learning-guided approach for executing arbitrary code snippets in an underconstrained way. 
All the python projects in the benchmark are open source projects and available on GitHub \cite{github} including the analysis frameworks and LExecutor.
The chosen projects are popular and belong to diverse application domains such as audio e.g., pydub, command-line tools e.g., thefuck, and computer vision e.g., scikit-learn.
The benchmark provided in this work provides a single command line access interface to run test suites and analysis.
The access interface provides the necessary functionality to execute dynamic analysis through DynaPyt, produce trace files via LExecutor, and generate static call graphs using PyCG.
Overall, this work aims to create a benchmark that achieves the properties of (1) large-scale which means large number of projects, (2) having diverse set of open source projects, (3) ready to run with a single interface, (4) ready-to-analyze dynamic features of projects, (5) compositional which means able to work with subset of projects, (6) long-term availability for usage and (7) extensible to add new projects and analyses. 

Developing a benchmark that meets the aforementioned properties is a challenging endeavor that presents various obstacles. The following paragraphs discusses some of the challenges encountered during the development process and outlines the strategies implemented to overcome them.

One major challenge for DyPyBench is to incorporate a large and diverse set of projects from various application domains.
Although numerous Python projects satisfying the criteria are available on GitHub, the benchmark can only accommodate a limited number of them.
% Moreover, GitHub lacks a classification system for projects according to their respective domains.
To address this limitation, we apply rigorous selection criteria to choose projects from a predefined and classified list.
Additionally, we consider GitHub stars as a metric to filter out non-reputable projects and ensure the inclusion of high-quality projects.

DyPyBench faces another hurdle in executing projects effectively, which is essential for its role as a benchmarking tool that relies on dynamic analysis.
The successful execution of projects is achieved through the execution of test suites for each project.
% Various methods exist for executing test suites, but DyPyBench has opted to utilize the widely adopted pytest library, which is capable of handling both pytest and unittest tests.
While not all projects include test suites that function flawlessly, it is crucial for test suites to run without errors to ensure the accurate operation of dynamic analysis tools.
DyPyBench overcomes this challenge by either skipping problematic test suites or fixing them to ensure proper execution.

% In order for DyPyBench to be ready-to-run, all projects must be installed and able to execute their respective test suites.
% However, the differing library dependencies among the various projects pose a significant challenge for DyPyBench.
% This obstacle is overcome by employing Python virtual environments that isolate each project from one another, thereby preventing any dependency conflicts.

Ensuring the accessibility and longevity of DyPyBench for users presents another challenge.
By packaging and exporting DyPyBench as a docker container, its accessibility is enhanced, as it eliminates operating system dependencies and provides a consistent, readily usable environment to all users.
This also overcomes the longevity challenge, as it facilitates the preservation of the projects and their respective environments.

The fluid nature of open-source projects often leads to regular modifications to their source code.
Such changes can affect the execution environment of a project, consequently impacting the effectiveness of DyPyBench.
To overcome this challenge, DyPyBench clones a stable and fixed version of the source code to ensure consistency in the execution environment.

In order to evaluate the dynamic nature of the benchmark, we run test suites and find that overall 91.83\% of test cases out of 45,086 execute successfully in 1362.56 seconds.
% In order to evaluate the usefulness of the benchmark, we use LExecutor and the analysis frameworks for two distinct applications.
For evaluation of the usefulness of the benchmark, we use LExecutor and the analysis frameworks for two distinct applications.
First, we provide a large amount of training data for the neural model of LExecutor in an attempt to improve the accuracy of the model.
DyPyBench is able to provide nearly 2.5 times the data compared to original training data used by the authors.
With this new training data, the neural model is able to predict the true value with an accuracy between 71.86\% and 93.67\% in different experimental setups.
Second, we provide some statistical analysis of call graphs generated using static and dynamic code analysis for Python.
PyCG generates static call graphs, whereas DynaPyt generates run-time call graphs in DyPyBench.
We find that static and dynamic call graphs have 3147 and 2938 common caller and callee entries respectively.
% Static call graphs always have more keys than dynamic call graphs.
DynaPyt has 66.41\% of callers present in PyCG, whereas PyCG has 51.93\% of callers present in DynaPyt.
For callees, DynaPyt has 28.85\% present in PyCG, wheras PyCG has 47.58\% present in DynaPyt.

In summary, this work contributes the following:
\begin{itemize}
    \item A python benchmark containing 50 projects from varied application domains.
    \item A benchmark with preinstalled frameworks for code analysis of python projects.
    \item A new and large dataset for training a neural model of LExecutor.
    \item A statistical analysis of static and run-time call graphs for Python.
    % \item A collection of static and dynamic call graphs to compare and analyze them.
\end{itemize}
