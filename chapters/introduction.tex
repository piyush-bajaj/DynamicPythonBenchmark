Python is a popular and versatile programming language that is distinguished by its interpreted nature, high-level syntax, and support for a variety of programming paradigms.
It makes use of dynamic typing and efficient memory management techniques, and its modular design makes it easy to extend and integrate programmable interfaces into pre-existing applications \cite{Python_language_wiki}.
Furthermore, Python provides a variety of dynamic features, such as object creation, attribute access, module loading, and function definition, making it appealing to developers of all skill levels. 

Recent studies have demonstrated Python's widespread usage in various applications, including analysis, bug detection, performance assessment, and vulnerability testing \cite{Python_usage_study1, Python_usage_study2}. This can be attributed to its accessible syntax, extensive community support, and comprehensive libraries. Python's robust support for data analysis, visualization, statistical analysis, and cross-platform compatibility renders it an exceptional choice for the development of cross-platform software and tools. Furthermore, Python is equipped with an assortment of testing and debugging tools, ranging from built-in frameworks like unittest and pytest to third-party tools such as PyCharm \cite{PyCharm_2023} and PyDev \cite{PyDev_2023} for debugging and code refactoring. Other such tools include Pynguin for test generation and DynaPyt for dynamic analysis. These tools aid developers in identifying and resolving bugs while optimizing code performance.

In the realm of high-level programming languages, Python, along with Java and C++, are frequently employed for their potency, object-oriented structure, and dynamic features, rendering them suitable for intricate software development applications. The dynamic features of Java encompass automatic memory management, garbage collection, reflections, and exceptions, while C++ offers templates, dynamic memory allocation, and runtime dynamic linking. To assess the dynamic features of these languages, dynamic benchmarks can be employed, which involve gauging the performance of a system or application under real-world conditions. Contrasting with static benchmarks that evaluate systems or applications under controlled circumstances, dynamic benchmarks endeavor to emulate actual usage patterns of users and applications to attain more accurate performance measurements. Java and C++ boast numerous dynamic benchmarks, such as Java Microbenchmark Harness (JMH) \cite{JMH}, The DaCapo Benchmark \cite{DaCapo_2006}, The Computer Language Benchmarks Game \cite{C++_Benchmark1}, and Boost.Benchmarks \cite{Boost_Benchmarks}. Although Python possesses some dynamic benchmarks, such as PyPerformance \cite{PyPerformance} and Apache Bench \cite{Apache_Benchmark}, they are not general-purpose benchmarks like those offered for Java and C++. Python benchmarks evaluate program performance for a single dynamic feature in a specific domain under real-world conditions.

While the existing python benchmarks evaluate single dynamic feature from a specific domain, a general-purpose benchmark would allow us to analyze the different dynamic features in the real world conditions. Such a benchmark would allow the evaluation of program performance from a wide range of application domains such as machine learning, web development, server developement and virtual environments, brought to the table due to the popularity and wide spread usage of python. With the general-purpose python dynamic benchmark one could generate run-time analysis using tools such as systrace \cite{systrace} and DynaPyt \cite{DynaPyt2022}. A general-purpose benchmark can provide training data for machine learning (ML) and deep learning (DL) tasks of software engineering such as bug detection \cite{DeepBugs2018}, code completion \cite{code_completion}, quality analysis \cite{Code_analysis_1, Code_analysis_2}, code refactoring \cite{code_refactoring}, and testing \cite{testing_1, testing_2, testing_3} which improve the productivity, efficiency and quality of code.

In this work, we provide a general-purpose dynamic benchmark of executable python software, named DyPyBench, which consists of 50 python projects including frameworks and libraries. The entire benchmark, encapsulated inside a docker container, can be setup using the docker image containing the projects, alongside the frameworks such as DynaPyt \cite{DynaPyt2022}, LExecutor \cite{LExecutor_2023} and PyCG \cite{PyCG_2021}. All of the python projects are open-sourced and available on github \cite{github} including the analysis frameworks mentioned above. The chosen projects are popular and belong to diverse application domains such as audio e.g., pydub, command-line tools e.g., thefuck, and computer vision e.g., scikit-learn. The benchmark provided in this work provides a single command line interface to run test suites and dynamic analysis using the DynaPyt framework. Using the same command line interface, DypyBench can run LExecutor to generate traces for training the neural model of LExecutor. PyCG, which helps in generating static call graphs can also be run using the command line interface. In essence this work aims to achieve the following goals for the dynamic benchmark:
\begin{itemize}
    \item \textbf{Large-scale} The benchmark should comprise tens of real-world open-source projects, allowing users to evaluate their analyses on a wide range of code.
    \item \textbf{Diverse} The benchmark should contain projects from a diverse range of application domains that reflect the state of today's Python ecosystem.
    \item \textbf{Ready-to-run} There should be a single interface to run each project in the benchmark, making it easy for users to set up and execute the entire benchmark.
    \item \textbf{Ready-to-analyze} To enable dynamic analyses, the executions of all projects in the benchmark should be set up to be analyzed.
    \item \textbf{Compositional} To help users understand the behavior of specific projects or even individual test cases, it should be easy to run subsets of full benchmark.
    \item \textbf{Long-term} The benchmark should be built using commonly used tools and formats, e.g., pip and Docker, to ensure its longevity.
    \item \textbf{**Extensibility} The benchmark should be easily extensible, allowing users to add, remove or update the tools, frameworks and projects.  
\end{itemize}

Python, being a general purpose language has been used to create projects in a wide variety of application domains such as machine learning, web development, and CLI development among others. Many of these projects are open sourced on GitHub and used by the community. DyPyBench, uses a subset of these projects which are popular and well accepted by the community of developers. In order to filter the popular and accepted projects from the large corpus, an open source project 'awesome-python' is used which provides us with nearly 500 projects from different categories which representing the application domains covered by python. Awesome-python has thus provided a large number of projects from diverse domains, however not all of these projects are well accepted in the community. This challenge is overcome by adding the criteria of a lower limit on GitHub stars of the project for filtering process.

As DyPyBench is a benchmarking tool that utilizes dynamic analysis, the proper execution of projects is a crucial feature. This is accomplished by running the test suites of each project. There are various methods for executing test suites, but DyPyBench utilizes the popular pytest library, which can handle both pytest and unittest tests. Consequently, the projects in DyPyBench can execute their test suites with the pytest library. To ensure that dynamic analysis tools function effectively, test suites must run successfully without errors. However, the existing test suites of some Python projects in DyPyBench contain failed test cases. To address this, DyPyBench either omits these test cases or modifies them by overwriting the corresponding code.

Each of the chosen projects in DyPyBench has unique requirements due to varying libraries and versions. To address this, our benchmark creates virtual environments for each project, installing the required dependencies as specified in the project's requirement file. For projects with additional library requirements, those libraries are added to their respective virtual environments with the help of conditions in bash scripts. We utilize a docker container with a Ubuntu base image to create a stable and consistent environment for the projects and their dependencies. This means that the benchmark can be easily reproduced and used in the long term without any conflicts or issues related to external dependencies or outdated libraries. The containerization ensures that the projects are ready to use and can be run reliably without any concerns about potential conflicts or dependencies.

In DyPyBench, the open-source projects are constantly being updated and enhanced, leading to changes in their source code and installation procedures. To address this issue, the benchmark sets a specific date for checking out the code from the GitHub repository, which ensures that the code used for the benchmark is consistent and reproducible regardless of any changes made in the future. Automating the installation, setup, and execution of each project in the benchmark can be challenging due to the large number of projects. To simplify this process, common steps are automated for all projects using bash scripts, while project-specific exceptions are specified where necessary.

DyPyBench offers several analysis tools and frameworks, each with their own unique way of execution, which can be confusing for users who need to know the specific command for each one. To simplify the process, DyPyBench has created a single command line interface that incorporates all of these tools and frameworks, making it easy to execute the necessary commands. This is achieved using Python script that can accept arguments for performing tasks related to a specific tool or framework.
