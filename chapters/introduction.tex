Python is a popular and versatile programming language that is distinguished by its interpreted nature, high-level syntax, and support for a variety of programming paradigms.
It makes use of dynamic typing and efficient memory management techniques, and its modular design makes it easy to extend and integrate programmable interfaces into pre-existing applications \cite{Python_language_wiki}.
Furthermore, Python provides a variety of dynamic features, such as object creation, attribute access, module loading, and function definition, making it appealing to developers of all skill levels. 

Recent studies have demonstrated Python's widespread usage in various applications, including analysis, bug detection, performance assessment, and vulnerability testing \cite{Python_usage_study1, Python_usage_study2, Python_usage3}.
This can be attributed to its accessible syntax, extensive community support, and comprehensive libraries.
Python's robust support for data analysis, visualization, statistical analysis, and cross-platform compatibility renders it an exceptional choice for the development of cross-platform software and tools.
Furthermore, Python is equipped with an assortment of testing and debugging tools, ranging from built-in frameworks like unittest and pytest to third-party tools such as PyCharm \cite{PyCharm_2023} and PyDev \cite{PyDev_2023} for debugging and code refactoring.
These tools aid developers in identifying and resolving bugs while optimizing code performance.

In the realm of high-level programming languages, Python, along with Java and C++, are frequently employed for their potency, object-oriented structure, and dynamic features, rendering them suitable for intricate software development applications. 
To assess the dynamic features of these languages, dynamic benchmarks can be employed, which involve gauging the performance of a system or application under real-world conditions.
Contrasting with static benchmarks that evaluate systems or applications under controlled circumstances, dynamic benchmarks endeavor to emulate actual usage patterns of users and applications to attain more accurate performance measurements.
Java and C++ boast numerous dynamic benchmarks, such as Java Microbenchmark Harness (JMH) \cite{JMH}, The DaCapo Benchmark \cite{DaCapo_2006}, The Computer Language Benchmarks Game \cite{C++_Benchmark1}, and Boost.Benchmarks \cite{Boost_Benchmarks}.
Although Python possesses some dynamic benchmarks, such as PyPerformance \cite{PyPerformance} and Apache Bench \cite{Apache_Benchmark}, they are not general-purpose benchmarks like those offered for Java and C++.
These python benchmarks evaluate program performance for a single dynamic feature in a specific domain.

While the existing python benchmarks evaluate single dynamic feature from a specific domain, a general-purpose benchmark would allow us to analyze the different dynamic features in the real world conditions.
Such a benchmark would allow the evaluation of program performance from a wide range of application domains such as machine learning, web development, server development and virtual environments, brought to the table due to the popularity and wide spread usage of python.
With the general-purpose python dynamic benchmark one could generate run-time analysis using tools such as systrace \cite{systrace} and DynaPyt \cite{DynaPyt2022}.
A general-purpose benchmark can provide training data for machine learning (ML) and deep learning (DL) tasks of software engineering such as bug detection \cite{DeepBugs2018}, code completion \cite{code_completion}, quality analysis \cite{Code_analysis_1, Code_analysis_2}, code refactoring \cite{code_refactoring}, and testing \cite{testing_1, testing_2, testing_3} which improve the productivity, efficiency and quality of code.

In this work, we provide such a general-purpose dynamic benchmark of executable python software. This benchmark is named DyPyBench and consists of 50 python projects including frameworks and libraries.
The entire benchmark, encapsulated inside a docker container, can be setup using the docker image containing the projects, alongside the analysis frameworks such as DynaPyt \cite{DynaPyt2022} and PyCG \cite{PyCG_2021}. Additionally, the benchmark also integrates LExecutor \cite{LExecutor_2023}, which is a learning-guided approach for executing arbitrary code snippets in an underconstrained way. 
All of the python projects are open-sourced and available on GitHub \cite{github} including the analysis frameworks and LExecutor.
The chosen projects are popular and belong to diverse application domains such as audio e.g., pydub, command-line tools e.g., thefuck, and computer vision e.g., scikit-learn.
The benchmark provided in this work provides a single command line access interface to run test suites and analysis.
The access interface provides the necessary functionality to execute dynamic analysis through DynaPyt, produce trace files via LExecutor, and generate static call graphs using PyCG.
This work aims to achieve the following properties for the dynamic benchmark:
\begin{itemize}
    \item Large-scale
    \item Diverse
    \item Ready-to-run
    \item Ready-to-analyze
    \item Compositional
    \item Long-term
    \item Extensibility
\end{itemize}

Developing a benchmark that meets the aforementioned properties is a challenging endeavor that presents various obstacles. The following paragraphs discusses some of the challenges encountered during the development process and outlines the strategies implemented to overcome them.

One major challenge for DyPyBench is to incorporate a large and diverse set of projects from various application domains.
Although numerous Python projects satisfying the criteria are available on GitHub, the benchmark can only accommodate a limited number of them.
% Moreover, GitHub lacks a classification system for projects according to their respective domains.
To address this limitation, we apply rigorous selection criteria to choose projects from a predefined and classified list.
Additionally, we consider GitHub stars as a metric to filter out non-reputable projects and ensure the inclusion of high-quality projects.

DyPyBench faces another hurdle in executing projects effectively, which is essential for its role as a benchmarking tool that relies on dynamic analysis.
The successful execution of projects is achieved through the execution of test suites for each project.
% Various methods exist for executing test suites, but DyPyBench has opted to utilize the widely adopted pytest library, which is capable of handling both pytest and unittest tests.
While not all projects include test suites that function flawlessly, it is crucial for test suites to run without errors to ensure the accurate operation of dynamic analysis tools.
DyPyBench overcomes this challenge by either skipping problematic test suites or fixing them to ensure proper execution.

% In order for DyPyBench to be ready-to-run, all projects must be installed and able to execute their respective test suites.
% However, the differing library dependencies among the various projects pose a significant challenge for DyPyBench.
% This obstacle is overcome by employing Python virtual environments that isolate each project from one another, thereby preventing any dependency conflicts.

Ensuring the accessibility and longevity of DyPyBench for users presents another challenge.
By packaging and exporting DyPyBench as a docker container, its accessibility is enhanced, as it eliminates operating system dependencies and provides a consistent, readily usable environment to all users.
This also overcomes the longevity challenge, as it facilitates the preservation of the projects and their respective environments.

The fluid nature of open-source projects often leads to regular modifications to their source code.
Such changes can affect the execution environment of a project, consequently impacting the effectiveness of DyPyBench.
To overcome this challenge, DyPyBench clones a stable and fixed version of the source code to ensure consistency in the execution environment.

In order to evaluate the dynamic nature of the benchmark, we run test suites and find that overall 91.83\% of test cases out of 45,086 execute successfully in 1362.56 seconds.
% In order to evaluate the usefulness of the benchmark, we use LExecutor and the analysis frameworks for two distinct applications.
For evaluation of the usefulness of the benchmark, we use LExecutor and the analysis frameworks for two distinct applications.
First, we provide a large amount of training data for the neural model of LExecutor in an attempt to improve the accuracy of the model.
DyPyBench is able to provide nearly 2.5 times the data compared to original training data used by the authors.
With this new training data, the neural model is able to predict the true value with an accuracy between 71.86\% and 93.67\% in different experimental setups.
Second, we provide some statistical analysis of call graphs generated using static and dynamic code analysis for Python.
PyCG generates static call graphs, whereas DynaPyt generates run-time call graphs in DyPyBench.
We find that static and dynamic call graphs have 3147 and 2938 common caller and callee entries respectively.
% Static call graphs always have more keys than dynamic call graphs.
DynaPyt has 66.41\% of callers present in PyCG, whereas PyCG has 51.93\% of callers present in DynaPyt.
For callees, DynaPyt has 28.85\% present in PyCG, wheras PyCG has 47.58\% present in DynaPyt.


% DyPyBench generates static call graphs using PyCG and run-time call graphs using DynaPyt and then compares these to find out the imprecision, matching and unmatched calls.
% **
% In this work, we utilize the benchmark we developed to demonstrate the effectiveness of each of the three incorporated analysis tools in three distinct applications.
% Firstly, LExecutor is utilized to generate a trace files, which are then used to develop a dataset for training and validating the neural network model used by the framework.
% Secondly, PyCG is used to produce a static call graph for the projects within DyPyBench, with the test files serving as the entry point for the execution.
% Lastly, by executing the test suites of the projects within DyPyBench with the assistance of DynaPyt, we generate dynamic call graphs.

% write some more on this evaluation
% DyPyBench and its applications in this work led to the following evaluation results. 
% DyPyBench was able to provide nearly 2.5 times the data for the LExecutor compared to the data used by the authors.
% Training and validation with 2.5x data provided us with an accuracy of 82 to 90 percentage for fine grained and coarse grained.
% The static call graphs and the dynamic call graphs generated were compared to get the following data. 

In summary, this work contributes the following:
\begin{itemize}
    \item A python benchmark containing 50 projects from varied application domains.
    \item A benchmark with preinstalled frameworks for code analysis of python projects.
    \item A new and large dataset for training a neural model of LExecutor.
    \item A statistical analysis of static and run-time call graphs for Python.
    % \item A collection of static and dynamic call graphs to compare and analyze them.
\end{itemize}
