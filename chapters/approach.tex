In this chapter, we describe the approach which we have adopted to create the dynamic benchmark of executable python software. The overall workflow of the approach is shown in the Figure \ref{fig:overall_approach}.

With the provided approach, our benchmark aims to achieve the properties of being large-scale, diverse, ready to run, ready to analyze, compositional and long term. We start with a large corpus of open source python projects listed in the awesome-python project (Section \ref{approach:corpus of python projects}). The projects are selected from this large corpus based on the selection criteria as described in Section \ref{approach:selection criteria}. This selection gives us a collection of python projects in the form of GitHub URLs and certain flags in a text file (Section \ref{approach:collection of projects}). Bash scripts use this collection of projects in order to automate the installation of projects and their dependencies (Section \ref{approach:bash scripts}) which provides a set of installed python projects and the environment for use by developers and researchers as described in Section \ref{approach:installed projects}. With the help of a python script (Section \ref{approach:python script}), a single command line interface provides easy access to the benchmark which consists of the installed projects along with various analysis tools and frameworks (Section \ref{approach:command line interface}). The benchmark containing the installed projects along with its command line interface is packaged and exported as a docker container for use by researchers and developers (Section \ref{approach: packing and exporting}). The command line interface incorporates  analysis frameworks like LExecutor, DynaPyt and PyCG into the benchmark for execution of various tasks such as dynamic analysis, trace file generation, and static call graph generation respectively (Section \ref{approach:analysis framework}).

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{figures/approach/DyPyBench.png}
\caption[Approach]{\label{fig:overall_approach}Overall Approach of DyPyBench.}
\end{figure}

\section{Corpus of Python projects}
\label{approach:corpus of python projects}
Python's ease of use and popularity, as well as the usage in different domains has led to a large number of projects being developed and made available to the community as open source software. GitHub alone contains more than 510000 public repositories that have python as their primary programming language \cite{bing_chat}(Bing Chat??). However, GitHub does not state which of these projects belong to which application domain. Also, it is difficult to go through 51000 repositories to filter out the projects from different domains without any classification.
% There are a large number of available python projects from a wide range of application domain for selection, however we need to classify them into specific domains.
This classification issue is solved with the help of Awesome-Python project, which is an open source project available on GitHub. This project contains a list of some of the most useful and interesting open source python projects including libraries, frameworks and software. Additionally, the awesome-python project classifies the python projects based on different categories which in essence represent the application domain of the project.
The awesome-python project acts as a corpus for our benchmark as it provides 679 python projects. It further classifies these projects into 92 main categories, out of which some of the categories can be further divided into sub-categories. Some of the categories and the number of projects in those are listed in table \ref{table:awesome-python}. For more details on the various categories and number of projects in each provided by the awesome-python project refer to the appendix \ref{appendix:awesome python projects}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lc}
    \hline
    \textbf{Category} & \textbf{Number of Projects}\\
    \hline
    Admin Panels    & 9\\
    Code analysis   & 18\\
    Computer vision & 7\\
    Deep Learning   & 7\\
    Documentation   & 3\\
    Files   & 7\\
    GUI development & 16\\
    Robotics    & 2\\
    Task Queues & 5\\
    Web Frameworks  & 5\\
    \hline
    \end{tabular}
    \caption{Project Categories (Awesome Python)}
    \label{table:awesome-python}
\end{table}

\section{Selection Criteria}
\label{approach:selection criteria}
The selection criteria enforces our benchmark to be large scale i.e., consisting of a large number of projects and diverse i.e., the projects must be from varied application domains. Github \cite{github} has a large number of open source projects written in the python programming languages which acts as a corpus for our benchmark. In order to select the projects which are well accepted in the community similar to the other accepted benchmarks such as ..., we add the criteria of a minimum threshold of stars on GitHub. We keep this threshold to be 1000 as too few stars would mean the project is not well accepted or is still pretty new, while 500 stars makes it popular but not so widely used. After filtering the projects based on GitHub stars, we select the projects from the corpus which belong to different application domains which are covered by the python projects. To name a few, the projects such as ... cover the domains ...respectively. Applying the selection criteria, we select 50 projects from the corpus of open source projects available on GitHub.

\section{Selected Python Projects}
\label{approach:selection of projects}
Since Python is a very popular and general purpose programming language, it has been used in many domains and we have a large set of open source python projects available on the internet. In this thesis we target projects from many of these varied domains which are well accepted in the community and at the same time open sourced. Another important factor is the availability of test suites in the project source code. 

\section{Setup and Install}
\label{approach:setup and install}
With a set of selected projects based on the required criteria, we then proceed to setup and install these projects with each one having its own virtual environment to install all the dependencies with their specific version from the requirements file. And then also using the source to install the project and its dependencies with pip package manager. Each of these projects are numbered and are placed inside their own folders. The projects source is cloned to a particular date. We also install some of the other python packages using pip in order to run the test suites successfully. We end the setup with installing the packages for testing and analysis tools present in the benchmark.

\section{Command Line Interface}
\label{approach:command line interface}
With the selected projects from section \ref{approach:selection of projects} installed and setup, we provide a command line interface for the user to run tests and dynamic analysis of a single project or a collection of projects. This command line interface is a single command with various available options. The various options available are to list all the installed projects with their urls, run the test suite of the project, perform instrumentation of the source code for dynamic analysis using dynapyt or lexecutor. execution of lexecurtor or dynapyt analysis. It also provides the option to update dynapyt and lexecutor. WE can also store the output to a particular file or spefic a timeout for the specific tasks where applicable. 

\section{Dynamic Analysis}
\label{approach:dynamic analysis}
While the command line interface provides us with varied options, one such option is the dynamic analysis. There are two available analysis, dynapyt and lexeecutor. We can use the command line to perform the specific analysis and generate the logs which we can use to train machine learning models for program analysis or use these logs on our own to understand the behaviour of the selected projects. These analysis can also help us in discovering some issues in the selected python projects. An example is dynapyt which explored a bug in pytorch. 

\section{ML Model}
\label{approach:ml model}

\section{Application}
\label{approach:application}