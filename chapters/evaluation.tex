In this chapter we discuss the experimental evaluation of DyPyBench. We do so by answering the following questions:

\textbf{RQ1:} How do the included test suites in Python projects within DyPyBench contribute to a dynamic benchmark?

\textbf{RQ2:} How efficient is our benchmark to generate (valid and good) training data for the neural models?

\textbf{RQ3:} How effective is DyPyBench in providing data to aid a comparison of static and dynamic call graph?

\section{RQ1: Effectiveness of Test Suite}
We find that DyPyBench provides 45086 test cases and the running time of test suites is between 0.02 seconds and 1362.86 seconds.
Overall, 41451 test cases pass where as 429 fail.
The successful execution of such a high number of test cases makes our benchmark dynamic in nature.
The number of projects which have 0 failed test cases in DyPyBench is 31, whereas 13 others have failed tests between 1 and 10.
The number of test cases in each project varies from 1 to 10552.
Figure \ref{fig:successful_tests} shows the percentage of passed test cases in all projects of DyPyBench.
We see that 41 projects have more than 90\% passed test cases, while 46 have 80\% or more passed test cases.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/evaluation/perc_passed_tests.png}
    \caption[Passed Test Case Percentage]{\label{fig:successful_tests}Percentage of Passed Tests in DyPyBench Projects}
\end{figure}

Since the execution of the project is important for dynamic analysis, the high pass percentage of 91.93\% for all tests cases is an advantage for efficient analysis.
The running time for each project is shown in Table \ref{fig:test_run_times}.
The total run time for all the projects in DyPyBench is 3568.86 seconds.
There are 39 projects which run the test suite within 50 seconds, whereas 4 projects have a run time greater than 200 seconds.
\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.4\linewidth,height=1.3\linewidth]{figures/evaluation/test_runtime.png}
    \includegraphics[width=1\linewidth]{figures/evaluation/tests_runtime2.png}
    \caption[Test Suite Run Time]{\label{fig:test_run_times}Test Suite Run Time in DyPyBench Projects}
\end{figure}

\paragraph{Overall, we find that the included test suites makes it easier to run the projects which is an important requirement to make a benchmark dynamic. The successful execution of a high number of test cases and the lower running time makes DyPyBench, a dynamic benchmark readily available for use.}

\section{RQ2: Effectiveness for Neural Model}
DyPyBench contributes 547,830 new data points for LExecutor. 
This results in the model accuracy between 71.86\% and 93.67\% after training the model.

The new data points we found are generated from 37 projects in our benchmark and amount to nearly 2.5 times the data points used by LExecutor.
The Figure \ref{fig:Lex_projects_events} shows the data points from each project labelled with project number.
As can be seen from the Figure \ref{fig:Lex_projects_events}, the distribution of these data points vary from project to project.
This can be attributed to the number of files in each project that are instrumented. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/evaluation/lex_data_points.png}
    \caption[Unique Value-use Events]{\label{fig:Lex_projects_events}Unique Value-use Events in DyPyBench Projects}
\end{figure}

To evaluate the accuracy of the neural model based on the new data points, we perform a total of 6 experiments using different validation data and the abstraction modes.
Overall, we use 3 different validation data-sets.
First is a random split of 5\% from the data points we found with 37 projects in DyPyBench, second is the data-set provided by the LExecutor. This data-set has data points which were not for training the model.
Finally, the third data-set is the split from the new data points by projects. 6 projects were selected at random, which constitute nearly 5\% of the data points and these projects were excluded from 
training data-set.
For each of the validation data-set we perform 2 experiments, one with the abstraction mode as fine-grained while the other with coarse-grained as described before.

\begin{figure}[ht]
    \centering
    \subfigure[Fine-grained]{\includegraphics[width=0.4\linewidth]{figures/evaluation/lex_random_fine.png}}
    \subfigure[Coarse-grained]{\includegraphics[width=0.4\linewidth]{figures/evaluation/lex_random_coarse.png}}
    \caption[Accuracy vs Epoch (Random Split)]{\label{fig:lex_random}Accuracy vs Epoch (Random Split) }
\end{figure}

Figure \ref{fig:lex_random} shows how the accuracy of the model varies with the number of epochs when the first validation data-set mentioned above is used.
Each line in the Figure \ref{fig:lex_random} shows the accuracy for top-n predictions for the same input variable.
As can be seen, the accuracy for top-5 is the best followed by top-4 and so on.
We mainly consider the accuracy of the first prediction, which starts at nearly 90\% and improves up to 93\% for both fine and coarse grained.

\begin{figure}[ht]
    \centering
    \subfigure[Fine-grained]{\includegraphics[width=0.4\linewidth]{figures/evaluation/lex_original_fine.png}}
    \subfigure[Coarse-grained]{\includegraphics[width=0.4\linewidth]{figures/evaluation/lex_original_coarse.png}}
    \caption[Accuracy vs Epoch (Original Data)]{\label{fig:lex_original}Accuracy vs Epoch (Original Data) }
\end{figure}

Similarly, Figure \ref{fig:lex_original} shows the accuracy versus epoch for validation data-set used by the LExecutor.
As we can see in the Figure \ref{fig:lex_original}, the accuracy is lower than the previous experiments plunging down to 73\% in both cases of abstraction.
Finally, the Figure \ref{fig:lex_project} shows the accuracy versus epoch comparison for the third validation data-set mentioned above.
In this case, the accuracy ranges between the previous two experiments having the value of 85\% for both levels of abstraction.
For the first and second validation data-set, we get the best model in the 7th and 8th epoch.
However, for the third data-set the best model is found at the 4th epoch itself.
Since, the neural model we start with is a pre-trained model, the accuracy range we found is similar to the one specified by LExecutor.
\begin{figure}[ht]
    \centering
    \subfigure[Fine-grained]{\includegraphics[width=0.4\linewidth]{figures/evaluation/lex_project_fine.png}}
    \subfigure[Coarse-grained]{\includegraphics[width=0.4\linewidth]{figures/evaluation/lex_project_coarse.png}}
    \caption[Accuracy vs Epoch (Project Split)]{\label{fig:lex_project}Accuracy vs Epoch (Project Split) }
\end{figure}

\paragraph{The quantity and quality of data is an important aspect for training neural network models. The higher quantity does not guarantee the quality of data. DyPyBench, provides projects which generates a large quantity of data and we evaluate the quality by calculating the accuracy. We find DyPyBench to be a good and valid data generator for LExecutor based on the amount of data generated and the accuracy maintained by the model.}

\section{RQ3: Effectiveness for Analysis Frameworks}
We find that DyPyBench provides 10799 key data points from DynaPyt and PyCG combined for comparison of call graphs.
These key data points refer to the callers and are generated from 20 projects in DyPyBench.
From the above, 6060 data points belongs to PyCG, and 4739 are from PyCG.
With a manual verification of these data points, we find that PyCG provides a key for every entry in the list of callees, resulting in a higher number of keys in PyCG output.
However, when we compare the key data points in DynaPyt with PyCG there are no matches found.
Manually checking the this provides us the insight that the keys in PyCG have starting point as the root of the project while those in DynaPyt start from the root of the benchmark.
We fix this issue, by replacing the keys in DynaPyt with the appropriate values.
After the above fix, we get 3147 key data points present in both.
66.41\% of keys in DynaPyt match with PyCG, while 51.93\% in PyCG are present in DynaPyt.
Figure \ref{fig:caller counts}, shows the distribution of the keys amongst DynaPyt and PyCG.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\linewidth]{figures/evaluation/callercounts.png}
    \caption[Distribution of Callers]{\label{fig:caller counts}Distribution of Callers }
\end{figure}

Next, we compare the number of value data points, which represent the callees.
We only compare the callees which are a part of the matching callers or keys.
There are a total of 16493 value data points provided by DyPyBench for comparison, out of which 10318 belong to DynaPyt and 6175 to PyCG.% of callees for the same callers
The high number of values from DynaPyt is due to multiple callee entries of the same function, but from a different object.
PyCG cannot detect callees based on objects since, it generates the call graphs from static analysis.
Another factor that we saw from the manual inspection was the callee entries in DynaPyt for some private functions which PyCG is not able to provide. 
A direct string comparison of the value data points in DynaPyt with PyCG shows a match of 1691 values.
This match contributes to 16.38\% match from DynaPyt and 27.38\% from PyCG.
With further manual verification, we find that certain data points essentially representing the same values do not match with a direct string comparison.
Figure \ref{fig:unmatched_value_strings} shows the differences of string in value data points from DynaPyt and PyCG.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/evaluation/string_compare.png}
    \caption[Unmatched Same Callees]{\label{fig:unmatched_value_strings}Unmatched Same Callees }
\end{figure}

Since the callees shown above are essentially the same, we include these in our comparison.
The number of matches in the value data points is now increased to 2938, which significantly increases the percentage contribution of PyCG matches to 47.58\% and DynaPyt to 28.47\%. 10182
As mentioned before, DynaPyt contains multiple entries for callees with different objects, we remove duplicate entries of some of callees with similar patterns.
Firstly, we replace the pattern \verb|<decouple.AutoConfig object at 0x7f94786106a0>| by \textit{decouple.AutoConfig} and pop the duplicate entries of \verb|decouple.AutoConfig|.
Second, we replace the pattern \\\verb|BoundMethodWeakref(<tests.test_saferef._Sample1 object at 0x7f73338edb10>.x)| by \\\verb|BoundMethodWeakref(<tests.test_saferef._Sample1>.x)| and pop its duplicate entries.
The above modification of values data points leads to decrease in total data points to 16358 which is a decrease of 136 callees from DynaPyt.
This results in a slight increase in the matching percentage of callees from DynaPyt to 28.85\%.
% However, we see that there are still patterns which should be included but are not
Figure \ref{fig:callee_counts} shows the distribution of the values amongst DynaPyt and PyCG for the matched keys.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\linewidth]{figures/evaluation/calleecounts.png}
    \caption[Distribution of Callees]{\label{fig:callee_counts}Distribution of Callees}
\end{figure}

\paragraph{Static and Dynamic Analysis are important tools for code analysis. A comparison of the two provides us an indication of how efficient is one over the other. DyPyBench helps with a comparison of call graphs generated using static and dynamic analysis. Although, DyPyBench provides comparable data from both approaches for call graphs, there are still certain errors present which would futher increase the percentage of matching callees and callers.}
