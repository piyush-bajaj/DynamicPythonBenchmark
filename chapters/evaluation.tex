In this chapter we discuss the experimental evaluation of DyPyBench. We do so by answering the following questions:
\begin{itemize}
    \item What is the total time needed to run the installation of all projects and individual projects into the benchmark?
    \item What are the differences in installation of different projects, apart from libraries and dependencies? Is the flow similar or too much different.
    \item How much time is taken to run test suites of individual projects and all projects together? Are there differences in running the tests using pytest?
    \item Application analysis of DyPyBench using DynaPyt
    \begin{itemize}
        \item How does the dynamic and static call graphs vary for different python projects?
        \item Can we use the above to improve the static call graph generation ?
    \end{itemize}
    \item Application analysis of DyPyBench using LExecutor
    \begin{itemize}
        \item How much improvement can we bring to the LExecutor framework to run the unrunnable python code ?
        \item How many unique use events our project contributes to the LExecutor ? Contribution of each project and whole benchmark.
    \end{itemize}
\end{itemize}