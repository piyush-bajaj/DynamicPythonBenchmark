% @book{Strunk-ElementsOfStyle,
%   author = {William Strunk Jr. and E. B. White},
%   title = {The Elements of Style},
%   year = {2000},
% }
 
% @book{Aho86-Compilers,
%   author = {Alfred V. Aho and Ravi Sethi and Jeffrey D. Ullman},
%   title = {Compilers: principles, techniques, and tools},
%   publisher = {Addison-Wesley Longman Publishing Co., Inc.},
%   year = {1986},
%   isbn = {0-201-10088-6}
% }

% @inproceedings{HerlihyMoss1993-TransactionalMemory,
%   author = {Maurice Herlihy and J. Eliot B. Moss},
%   title = {Transactional Memory: Architectural Support For Lock-free Data Structures},
%   booktitle = {Proc. of the 20th Intl. Symp. on Computer Architecture (ISCA'93)},
%   year = {1993},
%   pages = {289-300},
% }

% @article{FraserHanson1992-CodeGenerator,
%   author = {Christopher Fraser and David Hanson and Todd Proebsting},
%   title = {Engineering a Simple, Efficient Code Generator Generator},
%   journal = {ACM Letters on Programming Languages and Systems},
%   year = {1992},
%   volume = {1},
%   pages = {213--226},
%   number = {3},
%   month = sep,
% }

% @techreport{ArnoldFink2004-ArchitectureandPolicy,
%   author = {Matthew Arnold and Stephen Fink and David Grove and Michael Hind
% 	and Peter F. Sweeney},
%   title = {Architecture and Policy for Adaptive Optimization in Virtual Machines},
%   institution = {IBM Research Report 23429},
%   year = {2004},
%   month = nov,
% }

@misc{Benchmarking_2022,
  rights={Creative Commons Attribution-ShareAlike License},
  url={https://en.wikipedia.org/w/index.php?title=Benchmarking&oldid=1127768447},
  abstractNote={Benchmarking is the practice of comparing business processes and performance metrics to industry bests and best practices from other companies. Dimensions typically measured are quality, time and cost. 
  Benchmarking is used to measure performance using a specific indicator (cost per unit of measure, productivity per unit of measure, cycle time of x per unit of measure or defects per unit of measure) resulting in a metric of performance that is then compared to others.Also referred to as “best practice benchmarking” or “process benchmarking”, this process is used in management in which organizations evaluate various aspects of their processes in relation to best-practice companies’ processes, usually within a peer group defined for the purposes of comparison. This then allows organizations to develop plans on how to make improvements or adapt specific best practices, usually with the aim of increasing some aspect of performance. Benchmarking may be a one-off event, but is often treated as a continuous process in which organizations continually seek to improve their practices.
  In project management benchmarking can also support the selection, planning and delivery of projects.In the process of best practice benchmarking, management identifies the best firms in their industry, or in another industry where similar processes exist, and compares the results and processes of those studied (the “targets”) to one’s own results and processes. In this way, they learn how well the targets perform and, more importantly, the business processes that explain why these firms are successful. According to National Council on Measurement in Education, benchmark assessments  are short assessments used by teachers at various times throughout the school year to monitor student progress in some area of the school curriculum. These also are known as interim government.
  In 1994, one of the first technical journals named Benchmarking: An International Journal was published.}, note={Page Version ID: 1127768447},
  journal={Wikipedia},
  year={2022},
  month={Dec},
  language={en},
  title={Benchmarking}
}

@misc{Python_language_wiki,
 rights={Creative Commons Attribution-ShareAlike License}, 
 url={https://en.wikipedia.org/w/index.php?title=Python_(programming_language)&oldid=1136880732}, abstractNote={Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a “batteries included” language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.Python consistently ranks as one of the most popular programming languages.},
 note={Page Version ID: 1136880732}, 
 journal={Wikipedia}, year={2023}, 
 month={Feb}, 
 language={en},
 title={Python (Programming Language)}
}

@article{DynaPyt2022, 
  title={DynaPyt: A Dynamic Analysis Framework for Python}, 
  abstractNote={Python is a widely used programming language that powers important application domains such as machine learning, data analysis, and web applications. For many programs in these domains it is consequential to analyze aspects like security and performance, and with Python’s dynamic nature, it is crucial to be able to dynamically analyze Python programs. However, existing tools and frameworks do not provide the means to implement dynamic analyses easily and practitioners resort to implementing an ad-hoc dynamic analysis for their own use case. This work presents DynaPyt, the first general-purpose framework for heavy-weight dynamic analysis of Python programs. Compared to existing tools for other programming languages, our framework provides a wider range of analysis hooks arranged in a hierarchical structure, which allows developers to concisely implement analyses. DynaPyt features selective instrumentation and execution modification as well. We evaluate our framework on test suites of 9 popular open-source Python projects, 1,268,545 lines of code in total, and show that it, by and large, preserves the semantics of the original execution. The running time of DynaPyt is between 1.2x and 16x times the original execution time, which is in line with similar frameworks designed for other languages, and 5.6\%–88.6\% faster than analyses using a built-in tracing API offered by Python. We also implement multiple analyses, show the simplicity of implementing them and some potential use cases of DynaPyt. Among the analyses implemented are: an analysis to detect a memory blow up in Pytorch programs, a taint analysis to detect SQL injections, and an analysis to warn about a runtime performance anti-pattern.}, 
  author={Eghbali, Aryaz and Pradel, Michael}, 
  year={2022}, 
  pages={12}, 
  language={en}
}

@misc{Machine_Learning, year={2022},
  month={Dec},
  language={en},
  title={Machine Learning},
  url={https://www.ibm.com/topics/machine-learning} }

@misc{Machine_Learning_decade, 
 title={A Golden Decade of Deep Learning: Computing Systems & Applications}, 
 url={https://direct.mit.edu/daed/article/151/2/58/110623/A-Golden-Decade-of-Deep-Learning-Computing-Systems} 
}

@article{DeepBugs2018,
  title={DeepBugs: A Learning Approach to Name-based Bug Detection},
  url={http://arxiv.org/abs/1805.11683}, 
  DOI={10.48550/arXiv.1805.11683}, 
  abstractNote={Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89\% and 95\%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68\% true positive rate) in real-world code.}, 
  note={arXiv:1805.11683 [cs]}, number={arXiv:1805.11683},
  publisher={arXiv}, 
  author={Pradel, Michael and Sen, Koushik}, 
  year={2018}, 
  month={Apr}
}

@article{Code_analysis_1,
  title={A systematic literature review of machine learning techniques for software maintainability prediction},
  volume={119}, ISSN={0950-5849}, 
  DOI={10.1016/j.infsof.2019.106214}, 
  abstractNote={Context Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process. 
  Objective
  The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models.
  Method
  The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018.
  Results
  We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely.
  Conclusion
  Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.}, 
  journal={Information and Software Technology}, 
  author={Alsolai, Hadeel and Roper, Marc}, 
  year={2020}, 
  month={Mar}, 
  pages={106214},
  language={en} 
}

@article{Code_analysis_2, 
 title={Machine learning techniques for code smell detection: A systematic literature review and meta-analysis}, 
 volume={108}, 
 ISSN={0950-5849}, 
 DOI={10.1016/j.infsof.2018.12.009}, 
 abstractNote={Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.}, journal={Information and Software Technology}, 
 author={Azeem, Muhammad Ilyas and Palomba, Fabio and Shi, Lin and Wang, Qing}, 
 year={2019}, 
 month={Apr}, 
 pages={115–138}, 
 language={en} 
}

@inproceedings{code_completion, 
 address={New York, NY, USA}, 
 series={ASE ’20}, 
 title={Multi-task learning based pre-trained language model for code completion}, 
 ISBN={978-1-4503-6768-4}, 
 url={https://doi.org/10.1145/3324884.3416591},
 DOI={10.1145/3324884.3416591}, 
 abstractNote={Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.}, 
 booktitle={Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering}, publisher={Association for Computing Machinery}, 
 author={Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi}, 
 year={2021}, 
 month={Jan}, 
 pages={473–485}, 
 collection={ASE ’20}
}

@article{code_refactoring, 
 title={The Effectiveness of Supervised Machine Learning Algorithms in Predicting Software Refactoring}, 
 volume={48}, 
 ISSN={1939-3520}, 
 DOI={10.1109/TSE.2020.3021736}, 
 abstractNote={Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers’ expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90 percent. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts.}, 
 number={4}, 
 journal={IEEE Transactions on Software Engineering}, 
 author={Aniche, Maurício and Maziero, Erick and Durelli, Rafael and Durelli, Vinicius H. S.}, 
 year={2022}, month={Apr}, 
 pages={1432–1450}
}

 @inproceedings{testing_1, 
 title={Artificial Intelligence Applied to Software Testing: A Literature Review}, 
 ISSN={2166-0727}, 
 DOI={10.23919/CISTI49556.2020.9141124}, 
 abstractNote={In the last few years Artificial Intelligence (AI) algorithms and Machine Learning (ML) approaches have been successfully applied in real-world scenarios like commerce, industry and digital services, but they are not a widespread reality in Software Testing. Due to the complexity of software testing, most of the work of AI/ML applied to it is still academic. This paper briefly presents the state of the art in the field of software testing, applying ML approaches and AI algorithms. The progress analysis of the AI and ML methods used for this purpose during the last three years is based on the Scopus Elsevier, web of Science and Google Scholar databases. Algorithms used in software testing have been grouped by test types. The paper also tries to create relations between the main AI approaches and which type of tests they are applied to, in particular white-box, grey-box and black-box software testing types. We conclude that black-box testing is, by far, the preferred method of software testing, when AI is applied, and all three methods of ML (supervised, unsupervised and reinforcement) are commonly used in black-box testing being the “clustering” technique, Artificial Neural Networks and Genetic Algorithms applied to “fuzzing” and regression testing.}, booktitle={2020 15th Iberian Conference on Information Systems and Technologies (CISTI)}, 
 author={Lima, Rui and da Cruz, António Miguel Rosado and Ribeiro, Jorge}, 
 year={2020}, 
 month={Jun}, 
 pages={1–6} }

@inproceedings{testing_2, 
  address={New York, NY, USA}, 
  series={ICSEW’20}, 
  title={Deep Learning for Software Defect Prediction: A Survey}, 
  ISBN={978-1-4503-7963-2}, 
  url={https://doi.org/10.1145/3387940.3391463}, 
  DOI={10.1145/3387940.3391463}, 
  abstractNote={Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs’ semantics and fault prediction features and make accurate predictions.}, 
  booktitle={Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops}, publisher={Association for Computing Machinery}, 
  author={Omri, Safa and Sinz, Carsten}, 
  year={2020}, 
  month={Sep}, 
  pages={209–214}, 
  collection={ICSEW’20}
}

@article{testing_3, 
 title={Machine Learning Testing: Survey, Landscapes and Horizons}, 
 volume={48}, 
 ISSN={1939-3520}, 
 DOI={10.1109/TSE.2019.2962027}, 
 abstractNote={This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.}, 
 number={1}, 
 journal={IEEE Transactions on Software Engineering}, 
 author={Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang}, 
 year={2022}, 
 month={Jan}, 
 pages={1–36}
}

@article{static_code_analysis,
  title={A Survey on Machine Learning Techniques for Source Code Analysis}, 
  url={http://arxiv.org/abs/2110.09610}, 
  abstractNote={The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 479 primary studies published between 2011 and 2021. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources.}, 
  note={arXiv:2110.09610 [cs]}, 
  number={arXiv:2110.09610}, publisher={arXiv}, author={Sharma, Tushar and Kechagia, Maria and Georgiou, Stefanos and Tiwari, Rohit and Vats, Indira and Moazen, Hadi and Sarro, Federica}, 
  year={2022}, 
  month={Sep}
}

@inproceedings{loglens, 
  title={LogLens: A Real-Time Log Analysis System}, 
  ISSN={2575-8411}, DOI={10.1109/ICDCS.2018.00105}, 
  abstractNote={Administrators of most user-facing systems depend on periodic log data to get an idea of the health and status of production applications. Logs report information, which is crucial to diagnose the root cause of complex problems. In this paper, we present a real-time log analysis system called LogLens that automates the process of anomaly detection from logs with no (or minimal) target system knowledge and user specification. In LogLens, we employ unsupervised machine learning based techniques to discover patterns in application logs, and then leverage these patterns along with the real-time log parsing for designing advanced log analytics applications. Compared to the existing systems which are primarily limited to log indexing and search capabilities, LogLens presents an extensible system for supporting both stateless and stateful log analysis applications. Currently, LogLens is running at the core of a commercial log analysis solution handling millions of logs generated from the large-scale industrial environments and reported up to 12096x man-hours reduction in troubleshooting operational problems compared to the manual approach.}, 
  booktitle={2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)}, 
  author={Debnath, Biplob and Solaimani, Mohiuddin and Gulzar, Muhammad Ali Gulzar and Arora, Nipun and Lumezanu, Cristian and Xu, JianWu and Zong, Bo and Zhang, Hui and Jiang, Guofei and Khan, Latifur}, 
  year={2018}, 
  month={Jul}, 
  pages={1052–1062}
}

@article{LExecutor_2023, 
 title={LExecutor: Learning-Guided Execution}, 
 url={http://arxiv.org/abs/2302.02343}, 
 DOI={10.48550/arXiv.2302.02343}, 
 abstractNote={Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. However, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, missing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 80.1\% and 94.2\%, allowing LExecutor to closely mimic real executions. As a result, the approach successfully executes significantly more code than any available technique, such as simply executing the code as-is. For example, executing the open-source code snippets as-is covers only 4.1\% of all lines, because the code crashes early on, whereas LExecutor achieves a coverage of 50.1\%.}, note={arXiv:2302.02343 [cs]}, 
 number={arXiv:2302.02343}, 
 publisher={arXiv}, 
 author={Souza, Beatriz and Pradel, Michael}, 
 year={2023}, 
 month={Feb}
}

@misc{awesome_python_github, 
  title={vinta/awesome-python: A curated list of awesome Python frameworks, libraries, software and resources}, 
  url={https://github.com/vinta/awesome-python/}
}

@misc{dh-virtualenv_github, 
  title={spotify/dh-virtualenv: Python virtualenvs in Debian packages}, 
  url={https://github.com/spotify/dh-virtualenv}
}

@misc{JMH, 
 title={Java Microbenchmark Harness (JMH)_2023}, 
 type={Java}, 
 rights={GPL-2.0}, 
 url={https://github.com/openjdk/jmh}, 
 abstractNote={https://openjdk.org/projects/code-tools/jmh}, 
 publisher={OpenJDK}, 
 year={2023}, 
 month={Feb}
}

@inproceedings{DaCapo_2006, 
  address={New York, NY, USA}, 
  series={OOPSLA ’06}, 
  title={The DaCapo benchmarks: java benchmarking development and analysis}, 
  ISBN={978-1-59593-348-5}, url={https://doi.org/10.1145/1167473.1167488}, 
  DOI={10.1145/1167473.1167488}, 
  abstractNote={Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.}, 
  booktitle={Proceedings of the 21st annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications}, publisher={Association for Computing Machinery}, author={Blackburn, Stephen M. and Garner, Robin and Hoffmann, Chris and Khang, Asjad M. and McKinley, Kathryn S. and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z. and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J. Eliot B. and Phansalkar, Aashish and Stefanović, Darko and VanDrunen, Thomas and von Dincklage, Daniel and Wiedermann, Ben}, 
  year={2006}, 
  month={Oct}, 
  pages={169–190}, 
  collection={OOPSLA ’06}
}

@misc{C++_Benchmark1,
 rights={Creative Commons Attribution-ShareAlike License}, 
 url={https://en.wikipedia.org/w/index.php?title=The_Computer_Language_Benchmarks_Game&oldid=1132931747}, 
 abstractNote={The Computer Language Benchmarks Game (formerly called The Great Computer Language Shootout) is a free software project for comparing how a given subset of simple algorithms can be implemented in various popular programming languages.
  The project consists of:

  A set of very simple algorithmic problems
  Various implementations to the above problems in various programming languages
  A set of unit tests to verify that the submitted implementations solve the problem statement
  A framework for running and timing the implementations
  A website to facilitate the interactive comparison of the results}, 
  note={Page Version ID: 1132931747}, journal={Wikipedia}, 
  year={2023}, 
  month={Jan}, 
  language={en}
}

@misc{Boost_Benchmarks,
 title={Benchmarks - 1.81.0},
  url={https://www.boost.org/doc/libs/1_81_0/libs/json/doc/html/json/benchmarks.html}
}

@misc{PyPerformance, 
  title={The Python Performance Benchmark Suite — Python Performance Benchmark Suite 1.0.6 documentation},
  url={https://pyperformance.readthedocs.io/}
}

@misc{Apache_Benchmark,
 title={ab - Apache HTTP server benchmarking tool - Apache HTTP Server Version 2.4},
 url={https://httpd.apache.org/docs/2.4/programs/ab.html}
}

@misc{systrace, 
  title={sys — System-specific parameters and functions}, 
  url={https://docs.python.org/3/library/sys.html}, 
  abstractNote={This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available. Citations C99, ISO/IEC 9899...}, 
  journal={Python documentation}
}

