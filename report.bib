@misc{Benchmarking_2022,
    title={Benchmark},
    rights={Creative Commons Attribution-ShareAlike License},
    url={https://en.wikipedia.org/w/index.php?title=Benchmarking&oldid=1127768447},
    abstractNote={Benchmarking is the practice of comparing business processes and performance metrics to industry bests and best practices from other companies. Dimensions typically measured are quality, time and cost. 
    Benchmarking is used to measure performance using a specific indicator (cost per unit of measure, productivity per unit of measure, cycle time of x per unit of measure or defects per unit of measure) resulting in a metric of performance that is then compared to others.Also referred to as “best practice benchmarking” or “process benchmarking”, this process is used in management in which organizations evaluate various aspects of their processes in relation to best-practice companies’ processes, usually within a peer group defined for the purposes of comparison. This then allows organizations to develop plans on how to make improvements or adapt specific best practices, usually with the aim of increasing some aspect of performance. Benchmarking may be a one-off event, but is often treated as a continuous process in which organizations continually seek to improve their practices.
    In project management benchmarking can also support the selection, planning and delivery of projects.In the process of best practice benchmarking, management identifies the best firms in their industry, or in another industry where similar processes exist, and compares the results and processes of those studied (the “targets”) to one’s own results and processes. In this way, they learn how well the targets perform and, more importantly, the business processes that explain why these firms are successful. According to National Council on Measurement in Education, benchmark assessments  are short assessments used by teachers at various times throughout the school year to monitor student progress in some area of the school curriculum. These also are known as interim government.
    In 1994, one of the first technical journals named Benchmarking: An International Journal was published.},
    note={Page Version ID: 1127768447},
    journal={Wikipedia},
    year={2022},
    month={Dec},
    language={en}
}

@misc{Benchmarking_2023,
    title={Benchmark (computing)},
    rights={Creative Commons Attribution-ShareAlike License},
    url={https://en.wikipedia.org/w/index.php?title=Benchmark_(computing)&oldid=1137083952},
    abstractNote={In computing, a benchmark is the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it.The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.
    Benchmarking is usually associated with assessing performance characteristics of computer hardware, for example, the floating point operation performance of a CPU, but there are circumstances when the technique is also applicable to software. Software benchmarks are, for example, run against compilers or database management systems (DBMS).
    Benchmarks provide a method of comparing the performance of various subsystems across different chip/system architectures.},
    note={Page Version ID: 1137083952},
    journal={Wikipedia},
    year={2023},
    month={Feb},
    language={en}
}


@misc{Python_language_wiki,
    rights={Creative Commons Attribution-ShareAlike License}, 
    url={https://en.wikipedia.org/w/index.php?title=Python_(programming_language)&oldid=1136880732}, abstractNote={Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a “batteries included” language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.Python consistently ranks as one of the most popular programming languages.},
    note={Page Version ID: 1136880732}, 
    journal={Wikipedia}, year={2023}, 
    month={Feb}, 
    language={en},
    title={Python (Programming Language)}
}

@inproceedings{DynaPyt2022,
    author = {Eghbali, Aryaz and Pradel, Michael},
    title = {DynaPyt: A Dynamic Analysis Framework for Python},
    year = {2022},
    isbn = {9781450394130},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3540250.3549126},
    doi = {10.1145/3540250.3549126},
    abstract = {Python is a widely used programming language that powers important application domains such as machine learning, data analysis, and web applications. For many programs in these domains it is consequential to analyze aspects like security and performance, and with Python’s dynamic nature, it is crucial to be able to dynamically analyze Python programs. However, existing tools and frameworks do not provide the means to implement dynamic analyses easily and practitioners resort to implementing an ad-hoc dynamic analysis for their own use case. This work presents DynaPyt, the first general-purpose framework for heavy-weight dynamic analysis of Python programs. Compared to existing tools for other programming languages, our framework provides a wider range of analysis hooks arranged in a hierarchical structure, which allows developers to concisely implement analyses. DynaPyt features selective instrumentation and execution modification as well. We evaluate our framework on test suites of 9 popular open-source Python projects, 1,268,545 lines of code in total, and show that it, by and large, preserves the semantics of the original execution. The running time of DynaPyt is between 1.2x and 16x times the original execution time, which is in line with similar frameworks designed for other languages, and 5.6\%–88.6\% faster than analyses using a built-in tracing API offered by Python. We also implement multiple analyses, show the simplicity of implementing them and some potential use cases of DynaPyt. Among the analyses implemented are: an analysis to detect a memory blow up in Pytorch programs, a taint analysis to detect SQL injections, and an analysis to warn about a runtime performance anti-pattern.},
    booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
    pages = {760–771},
    numpages = {12},
    keywords = {python, dynamic analysis},
    location = {Singapore, Singapore},
    series = {ESEC/FSE 2022}
}

@misc{Machine_Learning, year={2022},
    month={Dec},
    language={en},
    title={Machine Learning},
    url={https://www.ibm.com/topics/machine-learning}
}

@misc{Machine_Learning_decade, 
    title={A Golden Decade of Deep Learning: Computing Systems & Applications}, 
    url={https://direct.mit.edu/daed/article/151/2/58/110623/A-Golden-Decade-of-Deep-Learning-Computing-Systems} 
}

@article{DeepBugs2018,
    title={DeepBugs: A Learning Approach to Name-based Bug Detection},
    url={http://arxiv.org/abs/1805.11683}, 
    DOI={10.48550/arXiv.1805.11683}, 
    abstractNote={Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89\% and 95\%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68\% true positive rate) in real-world code.}, 
    note={arXiv:1805.11683 [cs]}, number={arXiv:1805.11683},
    publisher={arXiv}, 
    author={Pradel, Michael and Sen, Koushik}, 
    year={2018}, 
    month={Apr}
}

@article{Code_analysis_1,
    title={A systematic literature review of machine learning techniques for software maintainability prediction},
    volume={119}, ISSN={0950-5849}, 
    DOI={10.1016/j.infsof.2019.106214}, 
    abstractNote={Context Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process. 
    Objective
    The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models.
    Method
    The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018.
    Results
    We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely.
    Conclusion
    Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.}, 
    journal={Information and Software Technology}, 
    author={Alsolai, Hadeel and Roper, Marc}, 
    year={2020}, 
    month={Mar}, 
    pages={106214},
    language={en} 
}

@article{Code_analysis_2, 
    title={Machine learning techniques for code smell detection: A systematic literature review and meta-analysis}, 
    volume={108}, 
    ISSN={0950-5849}, 
    DOI={10.1016/j.infsof.2018.12.009}, 
    abstractNote={Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.}, journal={Information and Software Technology}, 
    author={Azeem, Muhammad Ilyas and Palomba, Fabio and Shi, Lin and Wang, Qing}, 
    year={2019}, 
    month={Apr}, 
    pages={115–138}, 
    language={en} 
}

@inproceedings{code_completion, 
    address={New York, NY, USA}, 
    series={ASE ’20}, 
    title={Multi-task learning based pre-trained language model for code completion}, 
    ISBN={978-1-4503-6768-4}, 
    url={https://doi.org/10.1145/3324884.3416591},
    DOI={10.1145/3324884.3416591}, 
    abstractNote={Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.}, 
    booktitle={Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering}, publisher={Association for Computing Machinery}, 
    author={Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi}, 
    year={2021}, 
    month={Jan}, 
    pages={473–485}, 
    collection={ASE ’20}
}

@article{code_refactoring, 
    title={The Effectiveness of Supervised Machine Learning Algorithms in Predicting Software Refactoring}, 
    volume={48}, 
    ISSN={1939-3520}, 
    DOI={10.1109/TSE.2020.3021736}, 
    abstractNote={Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers’ expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90 percent. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts.}, 
    number={4}, 
    journal={IEEE Transactions on Software Engineering}, 
    author={Aniche, Maurício and Maziero, Erick and Durelli, Rafael and Durelli, Vinicius H. S.}, 
    year={2022}, month={Apr}, 
    pages={1432–1450}
}

 @inproceedings{testing_1, 
     title={Artificial Intelligence Applied to Software Testing: A Literature Review}, 
     ISSN={2166-0727}, 
     DOI={10.23919/CISTI49556.2020.9141124}, 
     abstractNote={In the last few years Artificial Intelligence (AI) algorithms and Machine Learning (ML) approaches have been successfully applied in real-world scenarios like commerce, industry and digital services, but they are not a widespread reality in Software Testing. Due to the complexity of software testing, most of the work of AI/ML applied to it is still academic. This paper briefly presents the state of the art in the field of software testing, applying ML approaches and AI algorithms. The progress analysis of the AI and ML methods used for this purpose during the last three years is based on the Scopus Elsevier, web of Science and Google Scholar databases. Algorithms used in software testing have been grouped by test types. The paper also tries to create relations between the main AI approaches and which type of tests they are applied to, in particular white-box, grey-box and black-box software testing types. We conclude that black-box testing is, by far, the preferred method of software testing, when AI is applied, and all three methods of ML (supervised, unsupervised and reinforcement) are commonly used in black-box testing being the “clustering” technique, Artificial Neural Networks and Genetic Algorithms applied to “fuzzing” and regression testing.}, booktitle={2020 15th Iberian Conference on Information Systems and Technologies (CISTI)}, 
     author={Lima, Rui and da Cruz, António Miguel Rosado and Ribeiro, Jorge}, 
     year={2020}, 
     month={Jun}, 
     pages={1–6} 
}

@inproceedings{testing_2, 
    address={New York, NY, USA}, 
    series={ICSEW’20}, 
    title={Deep Learning for Software Defect Prediction: A Survey}, 
    ISBN={978-1-4503-7963-2}, 
    url={https://doi.org/10.1145/3387940.3391463}, 
    DOI={10.1145/3387940.3391463}, 
    abstractNote={Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs’ semantics and fault prediction features and make accurate predictions.}, 
    booktitle={Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops}, publisher={Association for Computing Machinery}, 
    author={Omri, Safa and Sinz, Carsten}, 
    year={2020}, 
    month={Sep}, 
    pages={209–214}, 
    collection={ICSEW’20}
}

@article{testing_3, 
    title={Machine Learning Testing: Survey, Landscapes and Horizons}, 
    volume={48}, 
    ISSN={1939-3520}, 
    DOI={10.1109/TSE.2019.2962027}, 
    abstractNote={This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.}, 
    number={1}, 
    journal={IEEE Transactions on Software Engineering}, 
    author={Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang}, 
    year={2022}, 
    month={Jan}, 
    pages={1–36}
}

@article{static_code_analysis,
    title={A Survey on Machine Learning Techniques for Source Code Analysis}, 
    url={http://arxiv.org/abs/2110.09610}, 
    abstractNote={The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 479 primary studies published between 2011 and 2021. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources.}, 
    note={arXiv:2110.09610 [cs]}, 
    number={arXiv:2110.09610}, publisher={arXiv}, author={Sharma, Tushar and Kechagia, Maria and Georgiou, Stefanos and Tiwari, Rohit and Vats, Indira and Moazen, Hadi and Sarro, Federica}, 
    year={2022}, 
    month={Sep}
}

@inproceedings{loglens, 
    title={LogLens: A Real-Time Log Analysis System}, 
    ISSN={2575-8411}, DOI={10.1109/ICDCS.2018.00105}, 
    abstractNote={Administrators of most user-facing systems depend on periodic log data to get an idea of the health and status of production applications. Logs report information, which is crucial to diagnose the root cause of complex problems. In this paper, we present a real-time log analysis system called LogLens that automates the process of anomaly detection from logs with no (or minimal) target system knowledge and user specification. In LogLens, we employ unsupervised machine learning based techniques to discover patterns in application logs, and then leverage these patterns along with the real-time log parsing for designing advanced log analytics applications. Compared to the existing systems which are primarily limited to log indexing and search capabilities, LogLens presents an extensible system for supporting both stateless and stateful log analysis applications. Currently, LogLens is running at the core of a commercial log analysis solution handling millions of logs generated from the large-scale industrial environments and reported up to 12096x man-hours reduction in troubleshooting operational problems compared to the manual approach.}, 
    booktitle={2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)}, 
    author={Debnath, Biplob and Solaimani, Mohiuddin and Gulzar, Muhammad Ali Gulzar and Arora, Nipun and Lumezanu, Cristian and Xu, JianWu and Zong, Bo and Zhang, Hui and Jiang, Guofei and Khan, Latifur}, 
    year={2018}, 
    month={Jul}, 
    pages={1052–1062}
}

@article{LExecutor_2023, 
    title={LExecutor: Learning-Guided Execution}, 
    url={http://arxiv.org/abs/2302.02343}, 
    DOI={10.48550/arXiv.2302.02343}, 
    abstractNote={Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. However, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, missing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 80.1\% and 94.2\%, allowing LExecutor to closely mimic real executions. As a result, the approach successfully executes significantly more code than any available technique, such as simply executing the code as-is. For example, executing the open-source code snippets as-is covers only 4.1\% of all lines, because the code crashes early on, whereas LExecutor achieves a coverage of 50.1\%.}, note={arXiv:2302.02343 [cs]}, 
    number={arXiv:2302.02343}, 
    publisher={arXiv}, 
    author={Souza, Beatriz and Pradel, Michael}, 
    year={2023}, 
    month={Feb}
}

@misc{awesome_python_github, 
    title={vinta/awesome-python: A curated list of awesome Python frameworks, libraries, software and resources}, 
    url={https://github.com/vinta/awesome-python/}
}

@misc{dh-virtualenv_github, 
    title={spotify/dh-virtualenv: Python virtualenvs in Debian packages}, 
    url={https://github.com/spotify/dh-virtualenv}
}

@misc{JMH, 
    title={Java Microbenchmark Harness (JMH)\_2023}, 
    type={Java}, 
    rights={GPL-2.0}, 
    url={https://github.com/openjdk/jmh}, 
    abstractNote={https://openjdk.org/projects/code-tools/jmh}, 
    publisher={OpenJDK}, 
    year={2023}, 
    month={Feb}
}

@inproceedings{DaCapo_2006, 
    address={New York, NY, USA}, 
    series={OOPSLA ’06}, 
    title={The DaCapo benchmarks: java benchmarking development and analysis}, 
    ISBN={978-1-59593-348-5}, url={https://doi.org/10.1145/1167473.1167488}, 
    DOI={10.1145/1167473.1167488}, 
    abstractNote={Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.}, 
    booktitle={Proceedings of the 21st annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications}, publisher={Association for Computing Machinery}, author={Blackburn, Stephen M. and Garner, Robin and Hoffmann, Chris and Khang, Asjad M. and McKinley, Kathryn S. and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z. and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J. Eliot B. and Phansalkar, Aashish and Stefanović, Darko and VanDrunen, Thomas and von Dincklage, Daniel and Wiedermann, Ben}, 
    year={2006}, 
    month={Oct}, 
    pages={169–190}, 
    collection={OOPSLA ’06}
}

@misc{C++_Benchmark1,
    title={The Computer Language Benchmarks Game},
    rights={Creative Commons Attribution-ShareAlike License}, 
    url={https://en.wikipedia.org/w/index.php?title=The_Computer_Language_Benchmarks_Game&oldid=1132931747}, 
    abstractNote={The Computer Language Benchmarks Game (formerly called The Great Computer Language Shootout) is a free software project for comparing how a given subset of simple algorithms can be implemented in various popular programming languages.
    The project consists of:
    A set of very simple algorithmic problems
    Various implementations to the above problems in various programming languages
    A set of unit tests to verify that the submitted implementations solve the problem statement
    A framework for running and timing the implementations
    A website to facilitate the interactive comparison of the results}, 
    note={Page Version ID: 1132931747}, journal={Wikipedia}, 
    year={2023}, 
    month={Jan}, 
    language={en}
    }

@misc{Boost_Benchmarks,
    title={Boost.Benchmarks},
    url={https://www.boost.org/doc/libs/1_81_0/libs/json/doc/html/json/benchmarks.html}
}

@misc{PyPerformance, 
    title={The Python Performance Benchmark Suite — Python Performance Benchmark Suite 1.0.6 documentation},
    url={https://pyperformance.readthedocs.io/}
}

@misc{Apache_Benchmark,
    title={ab - Apache HTTP server benchmarking tool - Apache HTTP Server Version 2.4},
    url={https://httpd.apache.org/docs/2.4/programs/ab.html}
}

@misc{systrace, 
    title={sys — System-specific parameters and functions}, 
    url={https://docs.python.org/3/library/sys.html}, 
    abstractNote={This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available. Citations C99, ISO/IEC 9899...}, 
    journal={Python documentation}
}

@misc{github,
    title={GitHub},
    url={https://github.com},
    abstractNote={GitHub is where people build software. More than 100 million people use GitHub to discover, fork, and contribute to over 330 million projects.},
    journal={GitHub},
    language={en}
}

@misc{virtualenv,
    title={User Guide - virtualenv},
    url={https://virtualenv.pypa.io/en/latest/user_guide.html}
}

@misc{pip_package_manager,
    type={Python},
    title={pip: The PyPA recommended tool for installing Python packages.},
    rights={MIT License}, url={https://pip.pypa.io/},
    author={developers, The pip}
}

@misc{PyCharm_2023,
    title= {PyCharm},
    rights={Creative Commons Attribution-ShareAlike License}, 
    url={https://en.wikipedia.org/w/index.php?title=PyCharm&oldid=1147902283}, 
    abstractNote={PyCharm is an integrated development environment (IDE) used for programming in Python. It provides code analysis, a graphical debugger, an integrated unit tester, integration with version control systems, and supports web development with Django. PyCharm is developed by the Czech company JetBrains.It is cross-platform, working on Microsoft Windows, macOS and Linux. PyCharm has a Professional Edition, released under a proprietary license  and a Community Edition released under the Apache License. PyCharm Community Edition is less extensive than the Professional Edition.}, 
    note={Page Version ID: 1147902283}, 
    journal={Wikipedia}, 
    year={2023}, 
    month={Apr}, 
    language={en} 
}

@misc{PyDev_2023,
    title={PyDev},
    rights={Creative Commons Attribution-ShareAlike License},
    url={https://en.wikipedia.org/w/index.php?title=PyDev&oldid=1147368803}, 
    abstractNote={PyDev is a third-party plug-in for Eclipse. It is an Integrated Development Environment (IDE) used for programming in Python supporting code refactoring, graphical debugging, code analysis among other features.}, 
    note={Page Version ID: 1147368803}, 
    journal={Wikipedia}, 
    year={2023}, 
    month={Mar}, 
    language={en}
}

@article{PyCG_2021, 
    title={PyCG: Practical Call Graph Generation in Python}, 
    url={http://arxiv.org/abs/2103.00587}, 
    abstractNote={Call graphs play an important role in different contexts, such as profiling and vulnerability propagation analysis. Generating call graphs in an efficient manner can be a challenging task when it comes to high-level languages that are modular and incorporate dynamic features and higher-order functions. Despite the language’s popularity, there have been very few tools aiming to generate call graphs for Python programs. Worse, these tools suffer from several effectiveness issues that limit their practicality in realistic programs. We propose a pragmatic, static approach for call graph generation in Python. We compute all assignment relations between program identifiers of functions, variables, classes, and modules through an inter-procedural analysis. Based on these assignment relations, we produce the resulting call graph by resolving all calls to potentially invoked functions. Notably, the underlying analysis is designed to be efficient and scalable, handling several Python features, such as modules, generators, function closures, and multiple inheritance. We have evaluated our prototype implementation, which we call PyCG, using two benchmarks: a micro-benchmark suite containing small Python programs and a set of macro-benchmarks with several popular real-world Python packages. Our results indicate that PyCG can efficiently handle thousands of lines of code in less than a second (0.38 seconds for 1k LoC on average). Further, it outperforms the state-of-the-art for Python in both precision and recall: PyCG achieves high rates of precision ~99.2\%, and adequate recall ~69.9\%. Finally, we demonstrate how PyCG can aid dependency impact analysis by showcasing a potential enhancement to GitHub’s “security advisory” notification service using a real-world example.}, 
    note={arXiv:2103.00587 [cs]}, 
    number={arXiv:2103.00587}, 
    publisher={arXiv}, 
    author={Salis, Vitalis and Sotiropoulos, Thodoris and Louridas, Panos and Spinellis, Diomidis and Mitropoulos, Dimitris}, 
    year={2021}, 
    month={Feb} 
}

@misc{Why_Virtual_Env,
    title={Python Virtual Environments: A Primer – Real Python},
    url={https://realpython.com/python-virtual-environments-a-primer/},
    abstractNote={In this tutorial, you’ll learn how to use a Python virtual environment to manage your Python projects. You’ll also dive deep into the structure of virtual environments built using the venv module, as well as the reasoning behind using virtual environments.},
    author={Python, Real},
    language={en}
}

@misc{python_subprocess,
    title={subprocess — Subprocess management},
    url={https://docs.python.org/3/library/subprocess.html},
    abstractNote={Source code: Lib/subprocess.py The subprocess module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This module intends to replace seve...},
    journal={Python documentation}
}

@misc{Docker_2022,
    title={Docker},
    url={https://www.docker.com/},
    abstractNote={Docker is a platform designed to help developers build, share, and run modern applications. We handle the tedious setup, so you can focus on the code.},
    year={2022},
    month={May},
    language={en-US}
}

@misc{awesome_python_applications_github,
    type={Jupyter Notebook}, 
    title={Awesome Python Applications},
    url={https://github.com/mahmoud/awesome-python-applications},
    abstractNote={Free software that works great, and also happens to be open-source Python.},
    author={Hashemi, Mahmoud},
    year={2023},
    month={Apr}
}

@misc{python_projects,
    title={Python Projects},
    rights={MIT}, 
    url={https://github.com/practical-tutorials/project-based-learning},
    abstractNote={Curated list of project-based tutorials},
    publisher={practical-tutorials},
    year={2023},
    month={Apr}
}

@misc{docker_hub,
    title={Docker Hub},
    url={https://hub.docker.com/}
}

@inproceedings{Python_usage_study1,
    address={New York, NY, USA},
    series={FSE 2016},
    title={Python predictive analysis for bug detection},
    ISBN={978-1-4503-4218-6},
    url={https://dl.acm.org/doi/10.1145/2950290.2950357},
    DOI={10.1145/2950290.2950357},
    abstractNote={Python is a popular dynamic language that allows quick software development. However, Python program analysis engines are largely lacking. In this paper, we present a Python predictive analysis. It first collects the trace of an execution, and then encodes the trace and unexecuted branches to symbolic constraints. Symbolic variables are introduced to denote input values, their dynamic types, and attribute sets, to reason about their variations. Solving the constraints identifies bugs and their triggering inputs. Our evaluation shows that the technique is highly effective in analyzing real-world complex programs with a lot of dynamic features and external library calls, due to its sophisticated encoding design based on traces. It identifies 46 bugs from 11 real-world projects, with 16 new bugs. All reported bugs are true positives.},
    booktitle={Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    publisher={Association for Computing Machinery},
    author={Xu, Zhaogui and Liu, Peng and Zhang, Xiangyu and Xu, Baowen},
    year={2016},
    month={Nov},
    pages={121–132},
    collection={FSE 2016}
}

@misc{Python_usage_study2,
    title={Python Performance Testing},
    url={https://blog.sentry.io/2022/09/30/python-performance-testing-a-comprehensive-guide/},
    abstractNote={The following guest post addresses how to improve your services’s performance with Sentry and other application profilers for Python. Check…},
    journal={Product Blog • Sentry},
    year={2022},
    month={Sep},
    language={en}
}

@misc{Python_usage3,
    title={CVE-Search},
    type={Python},
    rights={AGPL-3.0},
    url={https://github.com/cve-search/cve-search},
    abstractNote={cve-search - a tool to perform local searches for known vulnerabilities},
    publisher={cve-search},
    year={2023},
    month={Apr}
}

@article{SPEC_C++_2006,
    title={C++ benchmarks in SPEC CPU2006},
    volume={35},
    ISSN={0163-5964},
    DOI={10.1145/1241601.1241617},
    abstractNote={In SPEC CPU2006, there are three C++ integer benchmarks and four floating-point C++ benchmarks. This paper describes the work of incorporating C++ benchmarks into SPEC CPU2006. It describes the base language standard supported and the basis for run rules adopted to maintain an even playing field for different compilers. It also describes issues that complicate porting C++ benchmarks. It describes some of the C++ Standard compliance issues that were technically interesting during the benchmark development phase, using as examples the behavior of const-correctness, nested class access of private member of enclosing class, and unneeded template instantiations.},
    number={1},
    journal={ACM SIGARCH Computer Architecture News},
    author={Wong, Michael},
    year={2007},
    month={Mar},
    pages={77–83}
    }

@inproceedings{SPEC_OMP_2012,
    address={Berlin, Heidelberg},
    series={Lecture Notes in Computer Science},
    title={SPEC OMP2012 — An Application Benchmark Suite for Parallel Systems Using OpenMP},
    ISBN={978-3-642-30961-8},
    DOI={10.1007/978-3-642-30961-8_17},
    abstractNote={This paper describes SPEC OMP2012, a benchmark developed by the SPEC High Performance Group. It consists of 15 OpenMP parallel applications from a wide range of fields. In addition to a performance metric based on the run time of the applications the benchmark adds an optional energy metric. The accompanying run rules detail how the benchmarks are executed and the results reported. They also cover the energy measurements. The first set of results provide scalability on three different platforms.},
    booktitle={OpenMP in a Heterogeneous World},
    publisher={Springer},
    author={Müller, Matthias S. and Baron, John and Brantley, William C. and Feng, Huiyu and Hackenberg, Daniel and Henschel, Robert and Jost, Gabriele and Molka, Daniel and Parrott, Chris and Robichaux, Joe and Shelepugin, Pavel and van Waveren, Matthijs and Whitney, Brian and Kumaran, Kalyan},
    editor={Chapman, Barbara M. and Massaioli, Federico and Müller, Matthias S. and Rorro, Marco},
    year={2012},
    pages={223–236},
    collection={Lecture Notes in Computer Science},
    language={en}
}

@article{SPEC_MPI_2007,
    author = {Müller, Matthias S. and van Waveren, Matthijs and Lieberman, Ron and Whitney, Brian and Saito, Hideki and Kumaran, Kalyan and Baron, John and Brantley, William C. and Parrott, Chris and Elken, Tom and Feng, Huiyu and Ponder, Carl},
    title = {SPEC MPI2007—an application benchmark suite for parallel systems using MPI},
    journal = {Concurrency and Computation: Practice and Experience},
    volume = {22},
    number = {2},
    pages = {191-205},
    keywords = {benchmark, MPI, performance},
    doi = {https://doi.org/10.1002/cpe.1535},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1535},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.1535},
    abstract = {Abstract The SPEC High-Performance Group has developed the benchmark suite SPEC MPI2007 and its run rules over the last few years. The purpose of the SPEC MPI2007 benchmark and its run rules is to further the cause of fair and objective benchmarking of high-performance computing systems. The rules help to ensure that the published results are meaningful, comparable to other results, and reproducible. MPI2007 includes 13 technical computing applications from the fields of computational fluid dynamics, molecular dynamics, electromagnetism, geophysics, ray tracing, and hydrodynamics. We describe the benchmark suite, and compare it with other benchmark suites. Copyright © 2009 John Wiley \& Sons, Ltd.},
    year = {2010}
}

@misc{software_benchmarks,
    title={How do you use software metrics and benchmarks to evaluate performance?},
    url={https://www.linkedin.com/advice/3/how-do-you-use-software-metrics-benchmarks},
    abstractNote={Software metrics and benchmarks are essential tools for evaluating the quality and performance of software products and processes. They can help you identify strengths, weaknesses, opportunities, and risks in your software development and delivery.},
    language={en}
}

@misc{dynamic_analysis,
    title={What Is Dynamic Analysis?},
    url={https://totalview.io/blog/what-dynamic-analysis},
    abstractNote={Find out why dynamic analysis is so important and how dynamic code analysis tools can simplify debugging in complex high-performance computing environments.},
    journal={TotalView by Perforce},
    language={en}
}

@article{github_stars,
    title={What’s in a GitHub Star? Understanding Repository Starring Practices in a Social Coding Platform},
    volume={146},
    ISSN={01641212},
    DOI={10.1016/j.jss.2018.09.016},
    abstractNote={GitHub developers star repositories mainly to show appreciation to the projects (52.5\%), to bookmark projects for later retrieval (51.1\%), and because they used or are using the projects (36.7\%).},
    journal={Journal of Systems and Software},
    author={Borges, Hudson and Tulio Valente, Marco},
    year={2018},
    month={Dec},
    pages={112–129},
    language={en}
}

@misc{static_code_analysis_tools,
    title={List of tools for static code analysis},
    rights={Creative Commons Attribution-ShareAlike License},
    url={https://en.wikipedia.org/w/index.php?title=List_of_tools_for_static_code_analysis&oldid=1149963228},
    abstractNote={This is a list of notable tools for static program analysis (program analysis is a synonym for code analysis).},
    note={Page Version ID: 1149963228},
    journal={Wikipedia},
    year={2023},
    month={Apr},
    language={en}
}

@article{Ball_1999,
    title={The concept of dynamic analysis},
    volume={24},
    ISSN={0163-5948},
    DOI={10.1145/318774.318944},
    abstractNote={Dynamic analysis is the analysis of the properties of a running program. In this paper, we explore two new dynamic analyses based on program profiling: Frequency Spectrum Analysis. We show how analyzing the frequencies of program entities in a single execution can help programmers to decompose a program, identify related computations, and find computations related to specific input and output characteristics of a program. Coverage Concept Analysis. Concept analysis of test coverage data computes dynamic analogs to static control flow relationships such as domination, postdomination, and regions. Comparison of these dynamically computed relationships to their static counterparts can point to areas of code requiring more testing and can aid programmers in understanding how a program and its test sets relate to one another.},
    number={6},
    journal={ACM SIGSOFT Software Engineering Notes},
    author={Ball, Thoms},
    year={1999},
    month={Nov},
    pages={216–234},
    language={en}
}

@inproceedings{Dynamic_analysis_usage,
    title={Detecting Cryptomining Using Dynamic Analysis},
    DOI={10.1109/PST.2018.8514167},
    abstractNote={With the rise in worth and popularity of cryptocurrencies, a new opportunity for criminal gain is being exploited and with little currently offered in the way of defence. The cost of mining (i.e., earning cryptocurrency through CPU-intensive calculations that underpin the blockchain technology) can be prohibitively expensive, with hardware costs and electrical overheads previously offering a loss compared to the cryptocurrency gained. Off-loading these costs along a distributed network of machines via malware offers an instantly profitable scenario, though standard Anti-virus (AV) products offer some defences against file-based threats. However, newer fileless malicious attacks, occurring through the browser on seemingly legitimate websites, can easily evade detection and surreptitiously engage the victim machine in computationally-expensive cryptomining (cryptojacking). With no current academic literature on the dynamic opcode analysis of cryptomining, to the best of our knowledge, we present the first such experimental study. Indeed, this is the first such work presenting opcode analysis on non-executable files. Our results show that browser-based cryptomining within our dataset can be detected by dynamic opcode analysis, with accuracies of up to 100\%. Further to this, our model can distinguish between cryptomining sites, weaponized benign sites, de-weaponized cryptomining sites and real world benign sites. As it is process-based, our technique offers an opportunity to rapidly detect, prevent and mitigate such attacks, a novel contribution which should encourage further future work.},
    booktitle={2018 16th Annual Conference on Privacy, Security and Trust (PST)},
    author={Carlin, Domhnall and O’Kane, Philip and Sezer, Sakir and Burgess, Jonah},
    year={2018},
    month={Aug},
    pages={1–6}
}

@inproceedings{newsome2005dynamic,
    title={Dynamic taint analysis for automatic detection, analysis, and signaturegeneration of exploits on commodity software.},
    author={Newsome, James and Song, Dawn Xiaodong},
    booktitle={NDSS},
    volume={5},
    pages={3--4},
    year={2005},
    organization={Citeseer}
}

@misc{Insta_architecture,
    title={Instagram Architecture \& Database – How Does It Store \& Search Billions Of Images},
    url={https://scaleyourapp.com/instagram-architecture-how-does-it-store-search-billions-of-images/},
    abstractNote={This write-up is a deep dive into the Instagram platform architecture & database. What technologies does it use on the backend? What database does it leverage? How does it search for content in the massive data it has? Let’s find out.},
    year={2019},
    month={May},
    language={en-US}
}

@misc{Spotify_python,
    title={How we use Python at Spotify},
    url={https://engineering.atspotify.com/2013/03/how-we-use-python-at-spotify/},
    abstractNote={How we use Python at Spotify - Spotify Engineering},
    journal={Spotify Engineering},
    author={Engineering, Spotify},
    year={2013},
    month={Mar},
    language={en-US}
}

@misc{game_python,
    title={Answer to “Famous games written in Python”},
    url={https://gamedev.stackexchange.com/a/5044},
    journal={Game Development Stack Exchange},
    year={2010},
    month={Oct}
}

@misc{python_scientific,
    title={Why Use Python in Scientific Computing?},
    url={https://www.datacamp.com/blog/the-case-for-python-in-scientific-computing},
    abstractNote={Discover the scope of Python for research, why scientists should use Python for scientific computing and how the Python community can aid scientific research.},
    language={en-US}
}

@misc{python_automate,
    title={Automate Everything With Python: A Comprehensive Guide to Python Automation},
    url={https://www.analyticsvidhya.com/blog/2023/04/python-automation-guide-automate-everything-with-python/},
    abstractNote={Learn how to automate tasks and streamline workflows in Python, everything from basic scripting to advanced automation techniques.}, journal={Analytics Vidhya},
    author={avcontentteam},
    year={2023},
    month={Apr},
    language={en}
}
